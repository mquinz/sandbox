{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Mobile Alarm Analytics v 3.0\n",
    "\n",
    "## version: 2020-10-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads Clusters identified during prior processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does not load individual alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "from time import process_time \n",
    "import http.client, urllib.parse\n",
    "\n",
    "\n",
    "from random import randint\n",
    "from py2neo import Graph\n",
    " \n",
    "communitiesFileName = '/Users/markquinsland/Documents/tmobile/alarm_analytics_v3/data/alarm_clusters_8-13.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample rows of input file\n",
    "\n",
    "``` text\n",
    "cluster,count,\"exec_time\",objects\n",
    "7,166,\"2020-04-22 00:06:18\",\"- SNMP SC SOAP Prov. Latency by AM Card::Mavenir_CDB\"\n",
    "7,166,\"2020-04-22 00:06:18\",\"/customer-loan/v1/loan-bundle-quotes: Health::Device-Finance\"\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to neo\n",
    "\n",
    "  \n",
    "try:\n",
    "    graph = Graph(\"bolt://localhost\", auth=(\"neo4j\", \"nimda\"))\n",
    "    print(\"Nodes: \", len(graph.nodes))\n",
    "    print(\"Relationships: \", len(graph.relationships))\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# indexes are in first map, constraints are in second map, TRUE will drop all existing constraints and indices\n",
    "# this API call does not currently support creating compound indexes such as Node Keys.  They must be created separately.\n",
    "\n",
    "schemaIndexCypherStmt = ''' \n",
    "\n",
    "CALL apoc.schema.assert(\n",
    "    {ManagerClass:['type','style'] }, {\n",
    "      Cluster:['id'],\n",
    "      TimeBucket:['time'],\n",
    "      Event:['id'],\n",
    "      AlarmClass:['id'],\n",
    "      ManagerClass:['id']\n",
    "      }, TRUE)\n",
    "'''\n",
    "graph.run(schemaIndexCypherStmt).to_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate node key constraints\n",
    "\n",
    "cypherStmt = ''' \n",
    "\n",
    "CREATE CONSTRAINT alarm_key\n",
    "ON (al:Alarm) ASSERT (al.id,al.time) IS NODE KEY;\n",
    "\n",
    "     \n",
    "'''\n",
    "graph.run(cypherStmt).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherStmt =  '''\n",
    "USING PERIODIC COMMIT 10000\n",
    "LOAD CSV WITH HEADERS FROM \"file:///''' + communitiesFileName + '''\" AS row\n",
    "// with row limit 100\n",
    "with row,split(row['objects'],\"::\") as parts, left (row.exec_time,10) as execDate\n",
    "\n",
    "// return parts [0],execDate, execDate + \"_\" +row['cluster']  as id\n",
    "MERGE (cl:Cluster {id:execDate + \"_\" +row['cluster'] })\n",
    "  set cl.idCount = toInteger(row.count)\n",
    " MERGE (ac:AlarmClass {id:parts[0]})\n",
    " MERGE (mc:ManagerClass {id:parts[1]})\n",
    " MERGE (ev:Event {id:row['objects']})\n",
    " MERGE (ev)<-[:HAS_EVENT]-(cl)\n",
    " MERGE (ev)-[:OF_CLASS]->(ac)\n",
    " MERGE (ac)-[:IS]->(mc)\n",
    "'''\n",
    "\n",
    "graph.run(cypherStmt).to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading individual alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAlarms (directoryName, fileName):\n",
    "    \n",
    "    \n",
    "    cypherStmt =  '''\n",
    "    // USING PERIODIC COMMIT 10000\n",
    "    LOAD CSV WITH HEADERS FROM \"file:///''' + directoryName + fileName + '''\" AS row\n",
    "    //with row limit 100\n",
    "    with row,CASE WHEN toInteger(row.created_date) > 15910100000 THEN toInteger(row.created_date) ELSE toInteger(row.created_date) * 1000 END  as ts\n",
    "    MERGE (al:Alarm {id: row['alert_name'] + \"::\" + row['manager_class'],time:datetime({ epochMillis: ts }) })   \n",
    "    set al.fileName = \"''' + fileName +      '''\", \n",
    "    al.timestamp = ts\n",
    "   \n",
    "   with al,row\n",
    "    \n",
    "    Match (ev:Event {id: row['alert_name'] + \"::\" + row['manager_class']})\n",
    "    MERGE (ev)<-[:IS]-(al)\n",
    "    return count(*) as count\n",
    "    '''\n",
    "\n",
    "    #print (cypherStmt)\n",
    "    count =  graph.run(cypherStmt).evaluate() or 0\n",
    "    print (fileName, count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAlarms_old (fileName):\n",
    "    cypherStmt =  '''\n",
    "    USING PERIODIC COMMIT 10000\n",
    "    LOAD CSV WITH HEADERS FROM \"file:///''' + alarmsDirectoryName + fileName + '''\" AS row\n",
    "    with row, toInteger(round(toInteger(row.created_date)/300000*300000  )) as timeBucketMillis\n",
    "\n",
    "    MERGE (tb:TimeBucket {time:datetime({ epochMillis: timeBucketMillis }) })   \n",
    "    with tb,row\n",
    "    Match (ev:Event {id: row['alert_name'] + \"::\" + row['manager_class']})\n",
    "    MERGE (ev)-[:IN]->(tb)\n",
    "    return count(*)\n",
    "    '''\n",
    "\n",
    "    graph.run(cypherStmt).to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Alarm files from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def files(path):\n",
    "    for file in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, file)):\n",
    "            yield file\n",
    "fileCount = 0\n",
    "rowCount = 0           \n",
    "for file in files(alarmsDirectoryName):\n",
    "    if file == '.DS_Store':\n",
    "        print (\"skip\", file)\n",
    "        continue\n",
    "    fileCount += 1\n",
    "    print (\"loading\", file)\n",
    "    count = loadAlarms(alarmsDirectoryName, file)\n",
    "    print (\"rows=\", count)\n",
    "    rowCount += count\n",
    "    \n",
    "print (\"total files\", fileCount)    \n",
    "print (\"total rows\", rowCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create time buckets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete existing timestamps  - optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cypherStmt =  ''' \n",
    "\n",
    "MATCH  (tb:TimeBucket )\n",
    "DETACH DELETE tb\n",
    "return   count(*) as bucketCount  \n",
    "'''\n",
    "\n",
    "\n",
    "print (graph.run(cypherStmt).to_table())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview Bucket Size \n",
    "### python dataframe example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketSizeMinutes = 5\n",
    "\n",
    "\n",
    "cypherStmt =  ''' \n",
    "\n",
    "\n",
    "MATCH (ev:Event)<-[:IS]-(al:Alarm) with ev, datetime({ epochMillis: al.time.epochMillis / $bucketMillis * $bucketMillis }) as bucketTime\n",
    "// MERGE (tb:TimeBucket {time:bucketTime})\n",
    "//MERGE (ev)-[:OCCURRED]->(tb)\n",
    "\n",
    "return bucketTime, count(*) as bucketSize order by bucketSize desc\n",
    "'''\n",
    "\n",
    "bucketMillis = bucketSizeMinutes * 60 * 1000\n",
    "eventStats = graph.run(cypherStmt,{'bucketMillis':bucketMillis}).to_data_frame()\n",
    "eventStats[[\"bucketSize\"]].describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the database with ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketSizeMinutes = 10\n",
    "\n",
    "\n",
    "cypherStmt =  ''' \n",
    "\n",
    "MATCH (ev:Event)<-[:IS]-(al:Alarm) with ev,al.time as alarmTime, datetime({ epochMillis: al.time.epochMillis / $bucketMillis * $bucketMillis }) as bucketTime\n",
    "MERGE (tb:TimeBucket {time:bucketTime})\n",
    "MERGE (ev)-[oc:OCCURRED]->(tb)\n",
    "    set oc.time = alarmTime\n",
    "\n",
    "return bucketTime, count(*) as bucketSize order by bucketSize desc\n",
    "'''\n",
    "\n",
    "bucketMillis = bucketSizeMinutes * 60 * 1000\n",
    "eventStats = graph.run(cypherStmt,{'bucketMillis':bucketMillis}).to_data_frame()\n",
    "eventStats[[\"bucketSize\"]].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cypherStmt = ''' \n",
    "    MATCH (e1:Event)-[:OCCURRED]->(tb:TimeBucket)<-[:OCCURRED]-(e2:Event) \n",
    "    WHERE exists ((e1)-[:CO_OCCURS]-(e2))\n",
    "    with  id(e1) as e1Id, id(e2) as e2Id, count(id(tb)) as sharedTimeBuckets\n",
    "    limit $limit\n",
    "    return *\n",
    "'''\n",
    " \n",
    "eventStats = graph.run(cypherStmt,{'limit':500}).to_data_frame()\n",
    "eventStats[[\"sharedTimeBuckets\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherStmt = ''' \n",
    "\n",
    "MATCH (cl:Cluster)-[:HAS_EVENT]->(ev:Event)\n",
    "return id(cl) as clusterId, count(distinct id(ev)) as events order by clusterId\n",
    " \n",
    " \n",
    "\n",
    "'''\n",
    "graph.run(cypherStmt ).to_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Event Sequence Info\n",
    "Using the timebuckets for any given time range, determine which events precede other event within each time buckets,\n",
    "then aggregate the counts.  \n",
    "\n",
    "Optionally use a minimum percentage in order to include a value. Set it to 0 to include all records.\n",
    "\n",
    "The first query obtains all events tied to a cluster and sorts them in chronological order - for each timebucket. This is important because it limits the number of potential events to just those that are in the cluster.   \n",
    "\n",
    "To process the results, the clusters are no longer important.  We're looking to see how often events preceed one another and the aggregation is at the event level, not the cluster level.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startDate = '2020-05-01'\n",
    "endDate = '2020-05-31'\n",
    "evPairs = {}\n",
    "evCounts = {}\n",
    "batch = []\n",
    "minPct = 20\n",
    "\n",
    "cypherStmt = ''' \n",
    "\n",
    "\n",
    "MATCH (cl:Cluster)-[:HAS_EVENT]->(ev:Event)-[r:OCCURRED]->(tb:TimeBucket)\n",
    "where datetime($startDate) <=tb.time <= datetime( $endDate)\n",
    "\n",
    "with * \n",
    "limit $limit\n",
    " with tb.time as tbTime, id(cl) as clusterId, id(ev) as eventId, r.time as evTime \n",
    "with * order by tbTime, clusterId, evTime  \n",
    "with tbTime, clusterId, collect(eventId) as events\n",
    "return  clusterId,  collect (events) as tbEvents\n",
    "\n",
    "'''\n",
    "results = graph.run(cypherStmt,{'startDate':startDate,'endDate':endDate,'limit':5000000})\n",
    "\n",
    "x = 0\n",
    "for record in results:\n",
    "    #if x>50:\n",
    "    #   break\n",
    "    x+=1\n",
    "    clusterId = record['clusterId']\n",
    "    tbEvents = record['tbEvents']\n",
    "    #print (x,'clustId',clusterId, 'tbEventsCount',len(tbEvents), tbEvents)\n",
    "    print (x,'clustId',clusterId, 'tbEventsCount',len(tbEvents) )\n",
    "    loadClusterSequences (clusterId, tbEvents, minimumPct)\n",
    "\n",
    "    \n",
    "    \n",
    "#print ('*********','evCounts', evCounts)\n",
    "for key in evPairs:\n",
    "    parts = key.split('_')\n",
    "    count = evPairs[key]\n",
    "    evCount =  evCounts[int(parts[0])]\n",
    "    # print ('*********','evCounts', evCounts)\n",
    "    matchPct = (evPairs[key] / evCounts[int(parts[0])]) * 100\n",
    "                                        \n",
    "    print ('key', key,'count',count, 'pct', matchPct)\n",
    "    if matchPct < minPct:\n",
    "        continue\n",
    "    \n",
    "    batch.append(\n",
    "        {'fromId':int(parts[0]),\n",
    "         'toId':int(parts[1]),\n",
    "         'count':evPairs[key], \n",
    "         'pct':matchPct, \n",
    "         'eventCount':evCounts[int(parts[0]) ]}\n",
    "         )\n",
    "\n",
    "print ('**** batch ',batch)\n",
    "\n",
    "createRelationshipsCypherStmt = '''  \n",
    "\n",
    " UNWIND $relationships as row\n",
    "    MATCH (fr:Event  ) where id(fr) = row['fromId']\n",
    "    MATCH (to:Event) where id(to) = row['toId']\n",
    "    MERGE (fr)-[rel:PRECEDES]->(to)\n",
    "        SET rel.precedesCount = row['count'],\n",
    "            rel.eventCount = row['eventCount'],\n",
    "            rel.pct =  toInteger() (row.precedesCount * 10000 / row.eventCount) /  100 )\n",
    "    return count(*) \n",
    "'''\n",
    "\n",
    "graph.run(createRelationshipsCypherStmt,{'relationships':batch}).to_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine cluster sequences for each timebucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadClusterSequences (clusterId, tbEvents, minPct):\n",
    "    \n",
    "   \n",
    "    \n",
    "    tbCount = len(tbEvents)\n",
    "    \n",
    "    global evPairs\n",
    "    global evCounts\n",
    "    global batch \n",
    "    \n",
    "    ''' \n",
    "    loop through all of the events in the timebucket.  they are sorted, oldest first.  for each event,\n",
    "    create a sequence pair for each of the events following it, not just the ones immediately following it.\n",
    "    This will help address problems like A,B,C then A,C,B - we want to know that both C and B follow A twice.\n",
    "    \n",
    "    '''\n",
    "    for tb in tbEvents:\n",
    "        tbSize = len(tb)\n",
    "        #print (tbCount, len(tb))\n",
    "        for x in range (0,tbSize):\n",
    "            #print (tbCount,tb[x], tbSize)\n",
    "            if tb[x] in evCounts:\n",
    "                evCounts [tb[x]] +=1\n",
    "            else:\n",
    "                evCounts [tb[x]]=1\n",
    "\n",
    "            if x+1 >= tbSize:\n",
    "                continue\n",
    "            key = str(tb[x])+ '_' + str(tb[x+1])\n",
    "            #print (\"** \",x, key)\n",
    "            if key in evPairs:\n",
    "                evPairs [key] +=1\n",
    "            else:\n",
    "                evPairs [key]=1\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherStmt = ''' \n",
    "    MATCH (e1:Event)-[:OCCURRED]->(tb:TimeBucket)<-[:OCCURRED]-(e2:Event) \n",
    "  \n",
    "    WHERE id(e1) > id(e2)\n",
    "      and exists ((e1)-[:CO_OCCURS]-(e2))\n",
    "    with  e1,e2, count(tb) as sharedTimeBuckets\n",
    "    //limit $limit\n",
    "   // MERGE (e1)-[rel:CO_OCCURS]-(e2)\n",
    "    // set rel.tb_count = sharedTimeBuckets\n",
    "    return * limit 100\n",
    "'''\n",
    " \n",
    "print(graph.run(cypherStmt).to_table())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older queries kept but not verified as still current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many events are in each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherStmt = ''' \n",
    "    MATCH (cl:Cluster)\n",
    "    return cl.idCount as eventCount \n",
    "'''\n",
    "clusterStats = graph.run(cypherStmt).to_data_frame()\n",
    "clusterStats[[\"eventCount\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the number of clusters each event belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherStmt = ''' \n",
    "    match (e:Event)<-[:HAS_EVENT]-(:Cluster)\n",
    "    with e,count(*) as clusterCount \n",
    "    set e.clusterCount = clusterCount\n",
    "    return e.id as eventid, clusterCount\n",
    "'''\n",
    "eventStats = graph.run(cypherStmt).to_data_frame()\n",
    "eventStats[[\"clusterCount\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the total number of siblings each event has - across all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total number of distinct events that co-occur (in any related cluster) with each event\n",
    "\n",
    "cypherStmt = '''  \n",
    "    MATCH (e1:Event)<-[:HAS_EVENT]-(c)-[:HAS_EVENT]->(e2)\n",
    "    with e1.id as eventId,  count(distinct e2.id) as siblingCount\n",
    "    return *\n",
    "'''\n",
    "\n",
    "siblingDF = graph.run(cypherStmt).to_data_frame()\n",
    "siblingDF[[\"siblingCount\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherStmt = ''' \n",
    "MATCH (e1:Event) <-[:HAS_EVENT]-(c)-[:HAS_EVENT]->(e2)\n",
    "    with e1, e1.clusterCount as clusterCount, e2, e2.clusterCount as e2ClusterCount, count(id(c)) as coCount\n",
    "   // with e1, clusterCount, e2, e2ClusterCount,count(cId) as coCount\n",
    "    where coCount > $minCoCount\n",
    "    with *, toInteger((toFloat(coCount)/clusterCount)*100) AS e1Pct, toInteger((toFloat(coCount)/e2ClusterCount)*100) AS e2Pct\n",
    "    where e1Pct >= $minPct or e2Pct >= $minPct\n",
    "    and id(e1) < id(e2)\n",
    "    return e1.clusterCount as e1Count, e1Pct, e2.clusterCount as e2Count, e2Pct, coCount, left(e1.id,50) as event1, left(e2.id,50) as event2   order by  coCount desc\n",
    "    limit $limit\n",
    "'''\n",
    "graph.run(cypherStmt,{'limit': 300, 'minPct':75, 'minCoCount':5}).to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CO_OCCURS Relationships\n",
    "These indicate that 2 event nodes are both in at least 1 cluster.   The number of co-occurrences will be stored as a property value called 'count'.   This property value can then be used to determine the co-occurrence percentage for each of the 2 nodes.  The higher the ratio of co-occurrences to the number of clusters (likely different for each of the 2 nodes), the higher the correlation is between the 2 events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypherStmt = ''' \n",
    "    match(:Event)-[rel:CO_OCCURS]-()\n",
    "    delete rel\n",
    "    return count (rel ) as deleted\n",
    "'''\n",
    "graph.run(cypherStmt).to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and store the correlation percentages of the relationships between events\n",
    "\n",
    "cypherStmt =  ''' \n",
    "MATCH (e1:Event) <-[:HAS_EVENT]-(c)-[:HAS_EVENT]->(e2)\n",
    "   where id(e1) < id(e2)\n",
    "    with e1, e2,count(c) as coCount\n",
    "    with *, (toFloat(coCount)/e1.clusterCount)*100 AS e1Pct, (toFloat(coCount)/e2.clusterCount)*100 AS e2Pct\n",
    "    where e1Pct >= $minPct and e2Pct >= $minPct \n",
    " merge (e1)-[rel:CO_OCCURS]->(e2)\n",
    "    set rel.count = coCount, rel.e1Pct = toInteger(e1Pct), rel.e2Pct = toInteger(e2Pct)\n",
    "RETURN count(*) as created\n",
    "'''\n",
    "graph.run(cypherStmt,{ 'minPct':60}).to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeBucket Queries\n",
    "\n",
    "## Description:  \n",
    "The goal of these queries is to determine which :Event nodes frequently occur.  \n",
    "\n",
    "Log files from approximately 30 days of activity have been provided for testing.  Each alarm record\n",
    "is linked to a :TimeBucket node after rounding the :Alarm time to the nearest 5 minutes.\n",
    "\n",
    "After loading the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships Beetween Clusters, Events, and Alarms\n",
    "\n",
    "<img src=\"resources/time_bucket_overview.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total number of distinct time buckets for each event\n",
    "\n",
    "cypherStmt = '''  \n",
    "    MATCH (e1:Event)-[:IN]-(tb) \n",
    "    with e1.id as eventId,  count(distinct id(tb)) as bucketCount\n",
    "    return *\n",
    "'''\n",
    "\n",
    "siblingDF = graph.run(cypherStmt).to_data_frame()\n",
    "siblingDF[[\"bucketCount\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of events linked to each timebucket\n",
    "\n",
    "cypherStmt = '''  \n",
    "    MATCH (e1:Event)-[:IN]->(tb) \n",
    "    with id(tb) as bucketId,  count(distinct id(e1)) as eventCount\n",
    "    return bucketId, eventCount\n",
    "'''\n",
    "\n",
    "siblingDF = graph.run(cypherStmt).to_data_frame()\n",
    "siblingDF[[\"eventCount\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the events that are in the same cluster and in the same time buckets\n",
    "\n",
    "cypherStmt = '''  \n",
    "    MATCH (e1:Event)-[:IN]->(tb)<-[:IN]-(e2:Event)\n",
    "    where exists( (e1) <-[:HAS_EVENT]-(:Cluster)-[:HAS_EVENT]->(e2))\n",
    "    return e1.id as ev1, e2.id as ev2, count(*) as count\n",
    "'''\n",
    "\n",
    "siblingDF = graph.run(cypherStmt).to_data_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblingDF[[\"count\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting data frame by name \n",
    "siblingDF.sort_values(\"count\", axis = 0, ascending = False, \n",
    "                 inplace = True)\n",
    "siblingDF[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
